{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "RANDOM_SEED = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# df.isnull().values.any()\n",
    "\n",
    "transactions = df.drop('Class', axis=1)\n",
    "labels = df['Class']\n",
    "\n",
    "# transactions['Time'] = np.log(transactions['Time'])\n",
    "# print(transactions.head())\n",
    "transactions['Time'] = np.log1p(transactions['Time'])   # log1p(x) = log(1+x) is used to avoid -inf values, since the 'Time' feature can be 0 and log(0) is -inf\n",
    "# (transactions['Amount'] == 0).any()\n",
    "transactions['Amount'] = np.log1p(transactions['Amount'])\n",
    "\n",
    "transactions = MinMaxScaler().fit_transform(transactions)\n",
    "\n",
    "legitimate_transactions = transactions[labels == 0]\n",
    "fraudulent_transactions = transactions[labels == 1]\n",
    "\n",
    "X_train, X_test = train_test_split(legitimate_transactions, test_size=0.05, random_state=RANDOM_SEED)\n",
    "\n",
    "y_train = np.zeros(X_train.shape[0])\n",
    "y_test = np.zeros(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - loss: 0.1538 - val_loss: 0.1049\n",
      "Epoch 2/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 0.1043 - val_loss: 0.1032\n",
      "Epoch 3/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 988us/step - loss: 0.1031 - val_loss: 0.1025\n",
      "Epoch 4/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 954us/step - loss: 0.1022 - val_loss: 0.1011\n",
      "Epoch 5/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.1010 - val_loss: 0.1007\n",
      "Epoch 6/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 925us/step - loss: 0.1005 - val_loss: 0.0995\n",
      "Epoch 7/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 8/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 9/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0987 - val_loss: 0.0985\n",
      "Epoch 10/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0986 - val_loss: 0.0986\n",
      "Epoch 11/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.0986 - val_loss: 0.0985\n",
      "Epoch 12/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0975 - val_loss: 0.0726\n",
      "Epoch 13/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.0724 - val_loss: 0.0724\n",
      "Epoch 14/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.0723 - val_loss: 0.0724\n",
      "Epoch 15/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0721 - val_loss: 0.0718\n",
      "Epoch 16/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 17/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 898us/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 18/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 874us/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 19/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 897us/step - loss: 0.0713 - val_loss: 0.0712\n",
      "Epoch 20/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 936us/step - loss: 0.0712 - val_loss: 0.0711\n",
      "Epoch 21/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0711 - val_loss: 0.0502\n",
      "Epoch 22/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0496 - val_loss: 0.0496\n",
      "Epoch 23/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0495 - val_loss: 0.0495\n",
      "Epoch 24/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 860us/step - loss: 0.0495 - val_loss: 0.0499\n",
      "Epoch 25/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 864us/step - loss: 0.0495 - val_loss: 0.0495\n",
      "Epoch 26/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 870us/step - loss: 0.0495 - val_loss: 0.0499\n",
      "Epoch 27/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0495 - val_loss: 0.0495\n",
      "Epoch 28/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 876us/step - loss: 0.0495 - val_loss: 0.0500\n",
      "Epoch 29/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - loss: 0.0495 - val_loss: 0.0494\n",
      "Epoch 30/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0494 - val_loss: 0.0496\n",
      "Epoch 31/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 856us/step - loss: 0.0495 - val_loss: 0.0494\n",
      "Epoch 32/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 909us/step - loss: 0.0494 - val_loss: 0.0496\n",
      "Epoch 33/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 907us/step - loss: 0.0494 - val_loss: 0.0494\n",
      "Epoch 34/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0494 - val_loss: 0.0497\n",
      "Epoch 35/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 826us/step - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 36/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 820us/step - loss: 0.0494 - val_loss: 0.0493\n",
      "Epoch 37/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 814us/step - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 38/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 804us/step - loss: 0.0494 - val_loss: 0.0496\n",
      "Epoch 39/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 815us/step - loss: 0.0494 - val_loss: 0.0496\n",
      "Epoch 40/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 812us/step - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 41/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 927us/step - loss: 0.0494 - val_loss: 0.0494\n",
      "Epoch 42/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0494 - val_loss: 0.0493\n",
      "Epoch 43/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 44/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 901us/step - loss: 0.0494 - val_loss: 0.0498\n",
      "Epoch 45/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 986us/step - loss: 0.0494 - val_loss: 0.0494\n",
      "Epoch 46/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 876us/step - loss: 0.0494 - val_loss: 0.0495\n",
      "Epoch 47/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0492 - val_loss: 0.0333\n",
      "Epoch 48/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 863us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 49/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 50/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 863us/step - loss: 0.0333 - val_loss: 0.0335\n",
      "Epoch 51/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 52/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 855us/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 53/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 970us/step - loss: 0.0333 - val_loss: 0.0336\n",
      "Epoch 54/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 821us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 55/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 759us/step - loss: 0.0333 - val_loss: 0.0334\n",
      "Epoch 56/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 854us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 57/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 843us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 58/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 59/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 60/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 749us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 61/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 803us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 62/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0333 - val_loss: 0.0334\n",
      "Epoch 63/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 64/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 65/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 759us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 66/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 748us/step - loss: 0.0333 - val_loss: 0.0333\n",
      "Epoch 67/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 741us/step - loss: 0.0333 - val_loss: 0.0332\n",
      "Epoch 68/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0332 - val_loss: 0.0334\n",
      "Epoch 69/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0333 - val_loss: 0.0335\n",
      "Epoch 70/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0333 - val_loss: 0.0334\n",
      "Epoch 71/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0333 - val_loss: 0.0335\n",
      "Epoch 72/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0332 - val_loss: 0.0334\n",
      "Epoch 73/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 822us/step - loss: 0.0332 - val_loss: 0.0333\n",
      "Epoch 74/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 796us/step - loss: 0.0332 - val_loss: 0.0333\n",
      "Epoch 75/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 785us/step - loss: 0.0332 - val_loss: 0.0333\n",
      "Epoch 76/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 833us/step - loss: 0.0332 - val_loss: 0.0333\n",
      "Epoch 77/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 789us/step - loss: 0.0332 - val_loss: 0.0333\n",
      "Epoch 78/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 766us/step - loss: 0.0304 - val_loss: 0.0159\n",
      "Epoch 79/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 785us/step - loss: 0.0158 - val_loss: 0.0157\n",
      "Epoch 80/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 796us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 81/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 781us/step - loss: 0.0157 - val_loss: 0.0159\n",
      "Epoch 82/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 83/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 770us/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 84/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 85/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 789us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 86/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 87/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 770us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 88/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 766us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 89/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755us/step - loss: 0.0157 - val_loss: 0.0157\n",
      "Epoch 90/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755us/step - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 91/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 92/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0156 - val_loss: 0.0158\n",
      "Epoch 93/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 833us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 94/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 800us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 95/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 766us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 96/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755us/step - loss: 0.0156 - val_loss: 0.0156\n",
      "Epoch 97/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 763us/step - loss: 0.0156 - val_loss: 0.0158\n",
      "Epoch 98/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0156 - val_loss: 0.0158\n",
      "Epoch 99/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 766us/step - loss: 0.0156 - val_loss: 0.0157\n",
      "Epoch 100/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 752us/step - loss: 0.0156 - val_loss: 0.0157\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(Model):\n",
    "  def __init__(self):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(15, activation=\"relu\"),\n",
    "      layers.Dense(7, activation=\"relu\")\n",
    "    ])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(15, activation=\"relu\"),\n",
    "      layers.Dense(30, activation=\"relu\")\n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "  \n",
    "autoencoder = Autoencoder()\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                          epochs=100,\n",
    "                          batch_size=64,\n",
    "                          validation_data=(X_test, X_test),\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8441/8441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 631us/step\n",
      "\u001b[1m460/460\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step\n"
     ]
    }
   ],
   "source": [
    "reconstructions_train = autoencoder.predict(X_train)\n",
    "train_loss = tf.keras.losses.mae(reconstructions_train, X_train)\n",
    "\n",
    "X_validation = np.concatenate((X_test, fraudulent_transactions), axis=0)\n",
    "y_validation = np.concatenate((y_test, np.ones(fraudulent_transactions.shape[0])), axis=0)\n",
    "\n",
    "reconstructions_validation = autoencoder.predict(X_validation)\n",
    "validation_loss = tf.keras.losses.mae(reconstructions_validation, X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives = 435\n",
      "False Positives = 1617\n",
      "True Negatives = 12599\n",
      "False Negatives = 57\n",
      "Accuracy = 0.8861843894479196\n",
      "Precision = 0.21198830409356725\n",
      "Recall = 0.8841463414634146\n",
      "F1 Score = 0.34198113207547176\n"
     ]
    }
   ],
   "source": [
    "# threshold = np.mean(train_loss)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "# threshold = np.percentile(train_loss, 90)\n",
    "# threshold = np.percentile(train_loss, 95)\n",
    "# threshold = np.percentile(train_loss, 99)\n",
    "\n",
    "predictions = tf.math.greater(validation_loss, threshold)\n",
    "\n",
    "print(\"True Positives = {}\".format(np.sum(np.logical_and(predictions, y_validation))))\n",
    "print(\"False Positives = {}\".format(np.sum(np.logical_and(predictions, np.logical_not(y_validation)))))\n",
    "print(\"True Negatives = {}\".format(np.sum(np.logical_and(np.logical_not(predictions), np.logical_not(y_validation)))))\n",
    "print(\"False Negatives = {}\".format(np.sum(np.logical_and(np.logical_not(predictions), y_validation))))\n",
    "\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_validation, predictions)))\n",
    "print(\"Precision = {}\".format(precision_score(y_validation, predictions)))\n",
    "print(\"Recall = {}\".format(recall_score(y_validation, predictions)))\n",
    "print(\"F1 Score = {}\".format(f1_score(y_validation, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 889us/step - loss: 0.0828 - val_loss: 0.0343\n",
      "Epoch 2/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0342 - val_loss: 0.0340\n",
      "Epoch 3/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0337 - val_loss: 0.0335\n",
      "Epoch 4/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 792us/step - loss: 0.0335 - val_loss: 0.0336\n",
      "Epoch 5/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 829us/step - loss: 0.0334 - val_loss: 0.0333\n",
      "Epoch 6/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 808us/step - loss: 0.0332 - val_loss: 0.0331\n",
      "Epoch 7/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step - loss: 0.0331 - val_loss: 0.0331\n",
      "Epoch 8/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 826us/step - loss: 0.0329 - val_loss: 0.0326\n",
      "Epoch 9/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 848us/step - loss: 0.0324 - val_loss: 0.0323\n",
      "Epoch 10/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 822us/step - loss: 0.0322 - val_loss: 0.0323\n",
      "Epoch 11/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 12/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 792us/step - loss: 0.0322 - val_loss: 0.0323\n",
      "Epoch 13/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 14/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 885us/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 15/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 914us/step - loss: 0.0322 - val_loss: 0.0323\n",
      "Epoch 16/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 989us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 17/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 18/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 19/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 881us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 20/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 21/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 22/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0322 - val_loss: 0.0324\n",
      "Epoch 23/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 24/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 855us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 25/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 848us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 26/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0321 - val_loss: 0.0323\n",
      "Epoch 27/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 866us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 28/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0321 - val_loss: 0.0321\n",
      "Epoch 29/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 844us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 30/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 815us/step - loss: 0.0321 - val_loss: 0.0321\n",
      "Epoch 31/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 826us/step - loss: 0.0321 - val_loss: 0.0321\n",
      "Epoch 32/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 863us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 33/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 815us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 34/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 822us/step - loss: 0.0321 - val_loss: 0.0323\n",
      "Epoch 35/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - loss: 0.0321 - val_loss: 0.0321\n",
      "Epoch 36/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 848us/step - loss: 0.0322 - val_loss: 0.0322\n",
      "Epoch 37/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 829us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 38/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0322 - val_loss: 0.0321\n",
      "Epoch 39/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 40/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 863us/step - loss: 0.0321 - val_loss: 0.0321\n",
      "Epoch 41/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 874us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 42/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 852us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 43/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 829us/step - loss: 0.0321 - val_loss: 0.0322\n",
      "Epoch 44/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811us/step - loss: 0.0321 - val_loss: 0.0320\n",
      "Epoch 45/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 829us/step - loss: 0.0321 - val_loss: 0.0320\n",
      "Epoch 46/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 47/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0320 - val_loss: 0.0321\n",
      "Epoch 48/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 867us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 49/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 826us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 50/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 818us/step - loss: 0.0320 - val_loss: 0.0321\n",
      "Epoch 51/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 859us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 52/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 992us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 53/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 837us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 54/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 55/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 939us/step - loss: 0.0320 - val_loss: 0.0321\n",
      "Epoch 56/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 57/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 876us/step - loss: 0.0320 - val_loss: 0.0320\n",
      "Epoch 58/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 915us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 59/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0319 - val_loss: 0.0321\n",
      "Epoch 60/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 887us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 61/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 893us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 62/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 784us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 63/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0319 - val_loss: 0.0321\n",
      "Epoch 64/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 770us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 65/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 781us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 66/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 67/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 68/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 69/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 755us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 70/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 869us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 71/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 768us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 72/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 770us/step - loss: 0.0319 - val_loss: 0.0321\n",
      "Epoch 73/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 74/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 789us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 75/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 763us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 76/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 791us/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 77/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 78/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 79/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 868us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 80/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 822us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 81/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 82/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 829us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 83/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 954us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 84/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 936us/step - loss: 0.0319 - val_loss: 0.0318\n",
      "Epoch 85/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 844us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 86/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 812us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 87/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 772us/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 88/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 840us/step - loss: 0.0318 - val_loss: 0.0318\n",
      "Epoch 89/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - loss: 0.0318 - val_loss: 0.0319\n",
      "Epoch 90/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 929us/step - loss: 0.0318 - val_loss: 0.0319\n",
      "Epoch 91/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 817us/step - loss: 0.0318 - val_loss: 0.0319\n",
      "Epoch 92/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 910us/step - loss: 0.0318 - val_loss: 0.0320\n",
      "Epoch 93/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 815us/step - loss: 0.0318 - val_loss: 0.0318\n",
      "Epoch 94/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 771us/step - loss: 0.0318 - val_loss: 0.0318\n",
      "Epoch 95/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 778us/step - loss: 0.0318 - val_loss: 0.0318\n",
      "Epoch 96/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 756us/step - loss: 0.0318 - val_loss: 0.0319\n",
      "Epoch 97/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 760us/step - loss: 0.0317 - val_loss: 0.0318\n",
      "Epoch 98/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 770us/step - loss: 0.0317 - val_loss: 0.0317\n",
      "Epoch 99/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 779us/step - loss: 0.0317 - val_loss: 0.0318\n",
      "Epoch 100/100\n",
      "\u001b[1m4221/4221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 764us/step - loss: 0.0317 - val_loss: 0.0318\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    mean, logvar = inputs\n",
    "    eps = tf.random.normal(shape=tf.shape(mean))\n",
    "    return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "class VAE(Model):\n",
    "  def __init__(self):\n",
    "    super(VAE, self).__init__()\n",
    "    \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Dense(15, activation=\"relu\"),\n",
    "      layers.Dense(7, activation=\"relu\"),\n",
    "      layers.Dense(2 + 2)\n",
    "    ])\n",
    "    \n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(15, activation=\"relu\"),\n",
    "      layers.Dense(30, activation=\"relu\")\n",
    "    ])\n",
    "    \n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "  \n",
    "  def reparameterize(self, mean, logvar):\n",
    "    return Sampling()([mean, logvar])\n",
    "  \n",
    "  def decode(self, z):\n",
    "    decoded = self.decoder(z)\n",
    "    return decoded\n",
    "\n",
    "  def call(self, x):\n",
    "    mean, logvar = self.encode(x)\n",
    "    z = self.reparameterize(mean, logvar)\n",
    "    decoded = self.decode(z)\n",
    "    return decoded\n",
    "\n",
    "vae = VAE()\n",
    "\n",
    "vae.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "history = vae.fit(X_train, X_train,\n",
    "                  epochs=100,\n",
    "                  batch_size=64,\n",
    "                  validation_data=(X_test, X_test),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8441/8441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 653us/step\n"
     ]
    }
   ],
   "source": [
    "reconstructions_train = vae.predict(X_train)\n",
    "train_loss = tf.keras.losses.mae(reconstructions_train, X_train)\n",
    "\n",
    "X_validation = np.concatenate((X_test, fraudulent_transactions), axis=0)\n",
    "y_validation = np.concatenate((y_test, np.ones(fraudulent_transactions.shape[0])), axis=0)\n",
    "\n",
    "reconstructions_validation = vae.predict(X_validation)\n",
    "validation_loss = tf.keras.losses.mae(reconstructions_validation, X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives = 443\n",
      "False Positives = 1872\n",
      "True Negatives = 12344\n",
      "False Negatives = 49\n",
      "Accuracy = 0.8693908077236878\n",
      "Precision = 0.1913606911447084\n",
      "Recall = 0.9004065040650406\n",
      "F1 Score = 0.3156394727467047\n"
     ]
    }
   ],
   "source": [
    "# threshold = np.mean(train_loss)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "# threshold = np.percentile(train_loss, 90)\n",
    "# threshold = np.percentile(train_loss, 95)\n",
    "# threshold = np.percentile(train_loss, 99)\n",
    "\n",
    "predictions = tf.math.greater(validation_loss, threshold)\n",
    "\n",
    "print(\"True Positives = {}\".format(np.sum(np.logical_and(predictions, y_validation))))\n",
    "print(\"False Positives = {}\".format(np.sum(np.logical_and(predictions, np.logical_not(y_validation)))))\n",
    "print(\"True Negatives = {}\".format(np.sum(np.logical_and(np.logical_not(predictions), np.logical_not(y_validation)))))\n",
    "print(\"False Negatives = {}\".format(np.sum(np.logical_and(np.logical_not(predictions), y_validation))))\n",
    "\n",
    "print(\"Accuracy = {}\".format(accuracy_score(y_validation, predictions)))\n",
    "print(\"Precision = {}\".format(precision_score(y_validation, predictions)))\n",
    "print(\"Recall = {}\".format(recall_score(y_validation, predictions)))\n",
    "print(\"F1 Score = {}\".format(f1_score(y_validation, predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
